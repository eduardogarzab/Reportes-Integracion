Este ejercicio de despliegue de un LLM en un entorno en la nube a través de Ollama y los contenedores de Docker nos permite obtener una perspectiva más aplicada de cómo funciona el ecosistema de la IA aplicado al desarrollo de software. En la industria principalmente su utilizan los LLMs corriéndolos en la nube, por esto es importante comprender cómo se pueden exponer estos modelos como servicios, consumirlos a través de APIs y mantener una arquitectura flexible con un desarrollo escalable conforme a las necesidades del proyecto, sobre todo cuando se trabaja en la nube con recursos bajo consumo.

Uno de los enfoques clave de Docker al trabajar con contenedores en la nube es la importancia de la portabilidad y la consistencia en los entornos de ejecución. Un contenedor puede funcionar en diferentes instancias sin que tengamos que preocuparnos por las dependencias, lo que facilita muchísimo la colaboración y reduce el número de errores. Esto considero que es muy importante con la tendencia que estamos viendo de tener infraestructura más dinámica y ligera, algo esencial para los proyectos de inteligencia artificial y microservicios en entornos cloud.

El uso de Ollama, en mi caso en particular con el modelo gemma3:1b, muestra cómo es posible aprovechar y utilizar modelos de lenguaje en la nube sin recurrir a instalaciones locales complejas. Al mismo tiempo, no solo se disminuye la dependencia de hardware específico, sino que también se puede considerar que es más flexible para escalar según la demanda del proyecto. Sin embargo, uno de los puntos en contra es que dependemos de la conectividad y de la administración de la nube para mantener el servicio estable y seguro.

En conclusión, este ejercicio demuestra la relevancia de unir conocimientos de infraestructura, programación y machine learning. Más allá de aprender todos los comandos y configuraciones, la verdadera enseñanza es reconocer cómo todo esto encaja en el panorama más amplio de la computación actual, donde los modelos de IA son solo una parte dentro de arquitecturas cada vez más integradas, escalables y orientadas a servicios en la nube.
