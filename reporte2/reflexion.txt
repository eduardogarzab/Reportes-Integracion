Este ejercicio de despliegue de un LLM en un entorno local a través de Ollama y los contenedores de Docker nos permite obtener una perspectiva más aplicada de cómo funciona el ecosistema de la IA aplicado al desarrollo de software. En la industria no solo se limita a usar los LLM simplemente corriéndolos en la nube, también es importante comprender cómo se pueden exponer estos modelos como servicios, consumirlos a través de APIs y mantener una arquitectura flexible con un desarrollo escalable conforme a las necesidades del proyecto.

Uno de los enfoques clave de Docker al trabajar con contenedores es la importancia de la portabilidad y la consistencia en los entornos de ejecución. Un contenedor puede funcionar en diferentes máquinas sin que tengamos que estar preocupándonos por las dependencias y demás, lo que facilita muchísimo la colaboración y reduce el número de errores. Esto considero que es muy importante con la tendencia que estamos viendo de tener infraestructura más dinámica y ligera, algo esencial para los proyectos de inteligencia artificial y microservicios.

El uso de Ollama, en mi caso en particular con el modelo gemma3:1b, muestra cómo es posible aprovechar y utilizar modelos de lenguaje locales sin recurrir a proveedores externos. Al mismo tiempo, no solo se disminuye la dependencia de terceros, sino que también se puede considerar que es más seguro al tratar con datos sensibles, ya que la información no sale de la infraestructura de un desarrollador u organización. Sin embargo, uno de los puntos en contra es que dependemos completamente de nuestro equipo para ejecutarlos por lo que si se trata de un modelo muy pesado es muy complicado que podamos correrlo de manera rápida en nuestras computadoras. Una de las complicaciones que se nos presentó fue el hecho de que el CORS (Cross-Origin Resource Sharing) no nos permitió reaizar las peticiones. Por lo que para resolver esta problemática, se ejecutó en un servidor localhost:8080 con python para poder hacer el bypass.
En conclusión, este ejercicio demuestra la relevancia de unir conocimientos de infraestructura, programación y machine learning. Más allá de aprender todos los comandos y configuraciones, la verdadera enseñanza es reconocer cómo todo esto encaja en el panorama más amplio de la computación actual, donde los modelos de IA son solo una parte dentro de arquitecturas cada vez más integradas, escalables y orientadas a servicios.
