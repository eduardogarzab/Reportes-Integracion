Este ejercicio ha sido una demostración fundamental de la diferencia crítica entre una aplicación que "funciona" y una que está "lista para producción". Al someter a prueba tres componentes distintos —un CMS, un microservicio de autenticación y un microservicio de catálogo — el ejercicio expuso cómo arquitecturas de desarrollo iniciales, que operan correctamente sin carga, pueden ser completamente inestables y fallar catastróficamente bajo la más mínima concurrencia. La lección inicial es contundente: las pruebas de rendimiento no son un paso opcional, sino un diagnóstico esencial para descubrir debilidades antes de que impacten a usuarios reales.

El proceso de diagnóstico fue tan valioso como la solución. Fue revelador identificar que los cuellos de botella más severos no siempre se deben a una lógica de negocio compleja, sino a configuraciones de infraestructura fundamentales. Por ejemplo, la falta de índices en una base de datos puede llevar los tiempos de respuesta de un inicio de sesión a más de 10 segundos , y el uso de un servidor de desarrollo de Flask en lugar de uno de producción como Gunicorn puede colapsar un servicio antes de que procese una sola solicitud. Esto subraya que el rendimiento del sistema es un esfuerzo holístico, donde la base de datos, el servidor de aplicaciones y el código deben estar optimizados para trabajar en conjunto.

Finalmente, este ejercicio redefinió el concepto de "fallo" del sistema. Las pruebas de break-point demostraron que el límite de una aplicación no siempre se manifiesta con errores evidentes (como HTTP 500), sino a través de una degradación catastrófica del rendimiento. Un sistema que responde en 22 segundos está, para fines prácticos, caído. Esto nos enseña la importancia vital de monitorear las métricas de latencia, como P95 y P99, ya que son el verdadero indicador de la experiencia del usuario y el primer síntoma de saturación del sistema.